import argparse
import json
import math
from pathlib import Path
from collections import Counter
from typing import List, Tuple, Optional, Dict
import re

import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# BERTScore ê´€ë ¨ import
try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    try:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "bert-score"])
        from bert_score import score as bert_score
        BERTSCORE_AVAILABLE = True
        print("Successfully installed and imported bert-score")
    except:
        BERTSCORE_AVAILABLE = False
        print("Warning: BERTScore not available. Installing bert-score...")

def word_tokens(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
    tokens = []
    cur = []
    for ch in text:
        if ch.isalpha():
            cur.append(ch.lower())
        else:
            if cur:
                tokens.append(''.join(cur))
                cur = []
    if cur:
        tokens.append(''.join(cur))
    return tokens

_SENT_SPLIT_RE = re.compile(r'(?<=[\.!?ã€‚ï¼ï¼Ÿ])\s+|[\r\n]+')

def sentence_split(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬"""
    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]
    if parts:
        return parts
    parts = []
    cur = []
    for ch in text:
        cur.append(ch)
        if ch in '.!?':
            parts.append(''.join(cur).strip())
            cur = []
    if cur:
        parts.append(''.join(cur).strip())
    return [s for s in parts if s]

def ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:
    """n-gram ìƒì„±"""
    if n <= 0:
        return []
    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_seg: int = 10) -> float:
    """MTLD (Measure of Textual Lexical Diversity) ê³„ì‚°"""
    def _mtld_one(seq: List[str]) -> float:
        types = set()
        token_count = 0
        factor_count = 0.0
        types_count = 0
        for tok in seq:
            token_count += 1
            if tok not in types:
                types.add(tok)
                types_count += 1
            ttr = types_count / token_count if token_count > 0 else 0.0
            if ttr <= ttr_threshold and token_count >= min_seg:
                factor_count += 1.0
                types.clear()
                token_count = 0
                types_count = 0
        if token_count > 0:
            ttr = types_count / token_count
            if (1 - ttr_threshold) > 0:
                factor_count += (1 - ttr) / (1 - ttr_threshold)
        return len(seq) / factor_count if factor_count > 0 else float(len(seq))
    
    if len(tokens) == 0:
        return 0.0
    return (_mtld_one(tokens) + _mtld_one(list(reversed(tokens)))) / 2.0

def mean_sentence_length(text: str) -> float:
    """í‰ê·  ë¬¸ì¥ ê¸¸ì´ ê³„ì‚°"""
    sents = sentence_split(text)
    if not sents:
        return float(len(word_tokens(text)))
    lengths = [len(word_tokens(s)) for s in sents]
    return float(np.mean(lengths)) if lengths else 0.0

def load_data_from_document(doc_content: str) -> pd.DataFrame:
    """ë¬¸ì„œì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜"""
    lines = doc_content.strip().split('\n')
    rows = []
    
    current_section = None
    for line in lines:
        line = line.strip()
        if line.startswith('< ') and line.endswith(' >'):
            current_section = line[2:-2].strip()
            continue
        
        if line.startswith('{') and line.endswith('}'):
            try:
                obj = json.loads(line)
                if obj.get("error") not in (None, ""):
                    continue
                cont = (obj.get("continuation") or "").strip()
                if not cont:
                    continue
                
                # ì„¹ì…˜ë³„ë¡œ method ê²°ì •
                if current_section == "dof":
                    method = "DoF"
                elif current_section == "zero-shot":
                    method = "Zero-shot"
                elif current_section == "few-shot":
                    method = "Few-shot"
                else:
                    continue
                
                rows.append({
                    "sample_id": int(obj.get("sample_id", -1)),
                    "method": method,
                    "continuation": cont,
                    "prompt_name": obj.get("prompt_name", "")
                })
            except Exception as e:
                print(f"Error parsing line: {line[:100]}... Error: {e}")
                continue
    
    if not rows:
        raise RuntimeError("No valid rows found in document.")
    
    return pd.DataFrame(rows)

def load_jsonl_from_directory(input_dir: Path) -> pd.DataFrame:
    """ë””ë ‰í† ë¦¬ì—ì„œ JSONL íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    files = sorted([p for p in input_dir.glob("*.jsonl") if p.is_file()])
    rows = []
    
    for file_path in files:
        print(f"Processing file: {file_path.name}")
        
        # íŒŒì¼ëª…ì—ì„œ method ì¶”ì¶œ
        filename = file_path.stem.lower()
        if "dof" in filename:
            method = "DoF"
        elif "zero" in filename or "0shot" in filename:
            method = "Zero-shot"  
        elif "few" in filename or "3shot" in filename:
            method = "Few-shot"
        else:
            print(f"Unknown method for file: {file_path.name}")
            continue
            
        with file_path.open("r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception as e:
                    print(f"JSON parse error in {file_path.name}:{line_num}: {e}")
                    continue
                    
                if obj.get("error") not in (None, ""):
                    continue
                    
                cont = (obj.get("continuation") or "").strip()
                if not cont:
                    continue
                    
                rows.append({
                    "sample_id": int(obj.get("sample_id", obj.get("index", -1))),
                    "method": method,
                    "continuation": cont,
                    "prompt_name": obj.get("prompt_name", ""),
                    "dof_value": obj.get("dof_value", None)
                })
    
    if not rows:
        raise RuntimeError("No valid rows found in directory.")
    
    return pd.DataFrame(rows)

def compute_bertscore(texts: List[str], references: List[str] = None, batch_size: int = 64) -> List[float]:
    """BERTScore F1 ì ìˆ˜ ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬)"""
    if not BERTSCORE_AVAILABLE:
        print("BERTScore not available, returning NaN values")
        return [np.nan] * len(texts)
    
    try:
        # ì°¸ì¡° í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°ë¡œ ì‚¬ìš©
        if references is None:
            references = [texts[0]] * len(texts)
        
        print(f"  Computing BERTScore for {len(texts)} samples...")
        
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        all_scores = []
        num_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in tqdm(range(0, len(texts), batch_size), 
                     desc="  BERTScore batches", 
                     total=num_batches,
                     leave=False):
            batch_texts = texts[i:i+batch_size]
            batch_refs = references[i:i+batch_size]
            
            # BERTScore ê³„ì‚° (F1 ì ìˆ˜ ì‚¬ìš©)
            P, R, F1 = bert_score(batch_texts, batch_refs, lang="en", verbose=False)
            all_scores.extend(F1.tolist())
        
        print(f"  âœ“ BERTScore computation completed")
        return all_scores
        
    except Exception as e:
        print(f"  âœ— Error computing BERTScore: {e}")
        return [np.nan] * len(texts)

def compute_metrics_per_sample(
    df: pd.DataFrame,
    show_progress: bool = True,
    use_bertscore: bool = True
) -> pd.DataFrame:
    """ê° ìƒ˜í”Œë³„ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    df = df.copy()
    
    # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    metrics = {"MTLD": [], "MSL": []}
    if use_bertscore and BERTSCORE_AVAILABLE:
        metrics["BERTScore"] = []
    
    iterator = tqdm(df.iterrows(), total=len(df), desc="Computing metrics") if show_progress else df.iterrows()
    
    # ê° ìƒ˜í”Œë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
    for _, row in iterator:
        txt = row["continuation"]
        toks = word_tokens(txt)
        
        # MTLD
        try:
            metrics["MTLD"].append(mtld(toks))
        except Exception:
            metrics["MTLD"].append(np.nan)
        
        # MSL
        try:
            metrics["MSL"].append(mean_sentence_length(txt))
        except Exception:
            metrics["MSL"].append(np.nan)
    
    # BERTScore ê³„ì‚° (ë°°ì¹˜ë¡œ ì²˜ë¦¬)
    if use_bertscore and BERTSCORE_AVAILABLE:
        print("\nğŸ“Š Computing BERTScore...")
        print("â„¹ï¸  Note: Using each method's first sample as reference")
        try:
            # ê° methodë³„ë¡œ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°ë¡œ ì‚¬ìš©
            method_groups = df.groupby("method")
            bertscore_values = []
            
            total_methods = len(method_groups)
            for method_idx, (method, group) in enumerate(method_groups, 1):
                group_texts = group["continuation"].tolist()
                if len(group_texts) > 0:
                    print(f"\n[{method_idx}/{total_methods}] Processing {method} method:")
                    print(f"  ğŸ“ {len(group_texts):,} samples to process")
                    
                    # í•´ë‹¹ methodì˜ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°ë¡œ ì‚¬ìš©
                    reference = group_texts[0]
                    references = [reference] * len(group_texts)
                    
                    scores = compute_bertscore(group_texts, references)
                    bertscore_values.extend(scores)
                    
                    # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
                    valid_scores = [s for s in scores if not np.isnan(s)]
                    if valid_scores:
                        print(f"  ğŸ“ˆ Mean BERTScore: {np.mean(valid_scores):.4f}")
                        print(f"  ğŸ“Š Score range: {np.min(valid_scores):.4f} - {np.max(valid_scores):.4f}")
                    
            metrics["BERTScore"] = bertscore_values
            print(f"\nâœ… BERTScore computation completed for all methods!")
            
        except Exception as e:
            print(f"\nâŒ Error computing BERTScore: {e}")
            metrics["BERTScore"] = [np.nan] * len(df)
    elif use_bertscore and not BERTSCORE_AVAILABLE:
        print("\nâš ï¸  BERTScore requested but not available")
        print("   Try installing with: pip install bert-score")
    else:
        print("\nâ­ï¸  BERTScore computation skipped (--no-bertscore flag)")
    
    # DataFrameì— ë©”íŠ¸ë¦­ ì¶”ê°€
    for metric_name, values in metrics.items():
        df[metric_name] = values
    
    return df

def plot_ecdf_comparison(df: pd.DataFrame, out_dir: Path, palette: Optional[List[str]] = None):
    """Methodë³„ ECDF ë¹„êµ í”Œë¡¯ ìƒì„±"""
    sns.set_style("whitegrid")
    
    methods = sorted(df["method"].unique())
    if palette is None:
        palette = sns.color_palette("Set2", n_colors=len(methods))
    
    # ì‚¬ìš©í•  ë©”íŠ¸ë¦­ë“¤
    metrics = ["MTLD", "MSL"]
    if "BERTScore" in df.columns:
        metrics.append("BERTScore")
    
    for metric in metrics:
        plt.figure(figsize=(10, 6))
        
        plot_df = df[["method", metric]].dropna().copy()
        if len(plot_df) == 0:
            continue
            
        for color, method in zip(palette, methods):
            method_data = plot_df[plot_df["method"] == method][metric].values
            if len(method_data) == 0:
                continue
            
            # ECDF í”Œë¡¯
            sns.ecdfplot(data=method_data, label=method, color=color, linewidth=2)
        
        plt.legend(title="Method", fontsize=12)
        plt.xlabel(metric, fontsize=12)
        plt.ylabel("Cumulative Probability", fontsize=12)
        plt.title(f"{metric} Distribution Comparison", fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ì €ì¥
        out_path = out_dir / f"ecdf_comparison_{metric}.png"
        plt.savefig(out_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Saved: {out_path}")

def generate_summary_stats(df: pd.DataFrame, out_dir: Path):
    """ìš”ì•½ í†µê³„ ìƒì„± ë° ì €ì¥"""
    metrics = ["MTLD", "MSL"]
    if "BERTScore" in df.columns:
        metrics.append("BERTScore")
    
    summary_stats = []
    
    for metric in metrics:
        for method in df["method"].unique():
            method_data = df[df["method"] == method][metric].dropna()
            if len(method_data) == 0:
                continue
                
            stats = {
                "Method": method,
                "Metric": metric,
                "Count": len(method_data),
                "Mean": method_data.mean(),
                "Std": method_data.std(),
                "Min": method_data.min(),
                "25%": method_data.quantile(0.25),
                "50%": method_data.quantile(0.50),
                "75%": method_data.quantile(0.75),
                "Max": method_data.max()
            }
            summary_stats.append(stats)
    
    stats_df = pd.DataFrame(summary_stats)
    
    # CSV ì €ì¥
    stats_path = out_dir / "summary_statistics.csv"
    stats_df.to_csv(stats_path, index=False, encoding="utf-8")
    print(f"Summary statistics saved: {stats_path}")
    
    # ì½˜ì†”ì— ì¶œë ¥
    print("\n=== Summary Statistics ===")
    for metric in metrics:
        print(f"\n{metric}:")
        metric_stats = stats_df[stats_df["Metric"] == metric]
        for _, row in metric_stats.iterrows():
            print(f"  {row['Method']}: Mean={row['Mean']:.3f}, Std={row['Std']:.3f}")

def create_combined_plot(df: pd.DataFrame, out_dir: Path):
    """ëª¨ë“  ë©”íŠ¸ë¦­ì„ í•œ ë²ˆì— ë³´ì—¬ì£¼ëŠ” ì¡°í•© í”Œë¡¯"""
    metrics = ["MTLD", "MSL"]
    if "BERTScore" in df.columns:
        metrics.append("BERTScore")
    
    fig, axes = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))
    if len(metrics) == 1:
        axes = [axes]
    
    methods = sorted(df["method"].unique())
    palette = sns.color_palette("Set2", n_colors=len(methods))
    
    for i, metric in enumerate(metrics):
        ax = axes[i]
        plot_df = df[["method", metric]].dropna().copy()
        
        for color, method in zip(palette, methods):
            method_data = plot_df[plot_df["method"] == method][metric].values
            if len(method_data) == 0:
                continue
            
            sns.ecdfplot(data=method_data, label=method, color=color, linewidth=2, ax=ax)
        
        ax.set_xlabel(metric, fontsize=12)
        ax.set_ylabel("Cumulative Probability", fontsize=12)
        ax.set_title(f"{metric}", fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        if i == 0:
            ax.legend(title="Method", fontsize=10)
    
    plt.tight_layout()
    combined_path = out_dir / "combined_ecdf_comparison.png"
    plt.savefig(combined_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Combined plot saved: {combined_path}")

def main():
    ap = argparse.ArgumentParser(description="Compare DoF, Zero-shot, and Few-shot performance using MTLD, MSL, and BERTScore metrics")
    ap.add_argument("--input-dir", required=True, help="Directory containing JSONL files")
    ap.add_argument("--out-dir", default=None, help="Output directory for plots and results")
    ap.add_argument("--no-bertscore", action="store_true", help="Skip BERTScore computation")
    args = ap.parse_args()
    
    input_dir = Path(args.input_dir)
    out_dir = Path(args.out_dir) if args.out_dir else (input_dir / "comparison_results")
    out_dir.mkdir(parents=True, exist_ok=True)
    
    print("Loading data...")
    try:
        df = load_jsonl_from_directory(input_dir)
        print(f"Loaded {len(df)} samples")
        print(f"Methods found: {df['method'].unique().tolist()}")
        print(f"Sample counts per method: {df['method'].value_counts().to_dict()}")
    except Exception as e:
        print(f"Error loading data: {e}")
        return
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    raw_csv = out_dir / "raw_data.csv"
    df.to_csv(raw_csv, index=False, encoding="utf-8")
    print(f"Raw data saved: {raw_csv}")
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    print("\nğŸ”„ Computing metrics...")
    print("ğŸ“Š Calculating MTLD (lexical diversity) and MSL (sentence length)...")
    use_bertscore = not args.no_bertscore
    df_metrics = compute_metrics_per_sample(df, show_progress=True, use_bertscore=use_bertscore)
    
    # ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ì¥
    metrics_csv = out_dir / "metrics_results.csv"
    df_metrics.to_csv(metrics_csv, index=False, encoding="utf-8")
    print(f"Metrics results saved: {metrics_csv}")
    
    # ECDF í”Œë¡¯ ìƒì„±
    print("\nğŸ“ˆ Generating ECDF plots...")
    plot_ecdf_comparison(df_metrics, out_dir)
    
    # ì¡°í•© í”Œë¡¯ ìƒì„±
    print("\nğŸ¨ Generating combined plot...")
    create_combined_plot(df_metrics, out_dir)
    
    # ìš”ì•½ í†µê³„ ìƒì„±
    print("\nğŸ“‹ Generating summary statistics...")
    generate_summary_stats(df_metrics, out_dir)
    
    print(f"\nğŸ‰ All results saved to: {out_dir}")
    print("\nğŸ“ Generated files:")
    print(f"   ğŸ“„ raw_data.csv: Original data")
    print(f"   ğŸ“Š metrics_results.csv: Data with computed metrics")
    print(f"   ğŸ“ˆ summary_statistics.csv: Statistical summary")
    print(f"   ğŸ–¼ï¸  ecdf_comparison_*.png: Individual metric comparisons")
    print(f"   ğŸ¨ combined_ecdf_comparison.png: All metrics in one plot")
    
    if not args.no_bertscore and BERTSCORE_AVAILABLE:
        print(f"\nâœ… BERTScore metric included in analysis")
    elif args.no_bertscore:
        print(f"\nâ­ï¸  BERTScore computation skipped (--no-bertscore flag)")
    else:
        print(f"\nâŒ BERTScore not available (install with: pip install bert-score)")

if __name__ == "__main__":
    main()